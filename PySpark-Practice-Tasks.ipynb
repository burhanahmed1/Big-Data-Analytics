{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inW_SieD3jaM",
        "outputId": "4cc37c5c-493d-4392-9890-4bc137bbe9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=52c6706f823748a64eccc1719f06c8b7c4d00469a46561ea7cbbb3e870a79726\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5R_nVQviw4N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZv5Vj56G5h"
      },
      "source": [
        "## **Q No 1 :**\n",
        "Find the total transaction amount for each customer for the year 2023. How would you approach\n",
        "this problem using Apache Spark?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucF53GfR4Vvs",
        "outputId": "7f4f34e2-10e4-4f4d-c6df-c1691c3669b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Q1.csv\n"
          ]
        }
      ],
      "source": [
        "%%file Q1.csv\n",
        "1,100,2023-01-05\n",
        "2,150,2023-01-10\n",
        "1,200,2023-02-15\n",
        "3,75,2023-03-20\n",
        "2,300,2023-04-25\n",
        "1,120,2023-05-30\n",
        "3,250,2023-06-05\n",
        "2,180,2023-07-10\n",
        "1,90,2023-08-15\n",
        "3,200,2023-09-20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN7gwb3W3zQQ"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q1.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],word[1]))\n",
        "\n",
        "wordcount = words2.reduceByKey(lambda x, y:int(x)  +int(y) )\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ1.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-QEOuZf7XXB"
      },
      "source": [
        "## **Q No 2 :**\n",
        "Find the total sales amount for each product. How would you approach this problem using\n",
        "Apache Spark?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHaiGAqf6Ewo",
        "outputId": "903b01f5-522f-4f67-9bf5-c4ff951da890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Q2.csv\n"
          ]
        }
      ],
      "source": [
        "%%file Q2.csv\n",
        "1,100.5\n",
        "2,75.25\n",
        "1,200.75\n",
        "3,150.0\n",
        "2,80.50\n",
        "3,120.25\n",
        "1,180.0\n",
        "2,90.75\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zoyg5BLJ8H4-"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q2.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],word[1]))\n",
        "\n",
        "wordcount = words2.reduceByKey(lambda x, y:float(x)  +float(y) )\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ2.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jteyk06ApgT"
      },
      "source": [
        "## **Q No 3 :**\n",
        "You have two datasets: employees and contractors. Each dataset contains information about\n",
        "individuals, including their name and department. You need to perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Employees.csv**\n",
        "\n",
        "+  name,department\n",
        "1. John,Engineering\n",
        "2. Alice,Sales\n",
        "3. Bob,Engineering\n",
        "4. Emma,Marketing\n",
        "\n",
        "**Contractors.csv**\n",
        "\n",
        "+  name,department\n",
        "1. Mike,Engineering\n",
        "2. Sarah,Marketing\n",
        "3. David,Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juyN5Y93fmN7"
      },
      "source": [
        "### **a)**\n",
        "Find all employees who work in the \"Engineering\" department.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skNw_UwSCcoR",
        "outputId": "107241c8-d38a-4028-efb0-a0e601321a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----------+\n",
            "|Name| Department|\n",
            "+----+-----------+\n",
            "|John|Engineering|\n",
            "| Bob|Engineering|\n",
            "+----+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Q3_1.com').getOrCreate()\n",
        "emp=[(\"John\",\"Engineering\"),\n",
        "      (\"Alice\",\"Sales\"),\n",
        "      (\"Bob\",\"Engineering\"),\n",
        "      (\"Emma\",\"Marketing\")]\n",
        "cols1=[\"Name\",\"Department\"]\n",
        "\n",
        "con=[(\"Mike\",\"Engineering\"),\n",
        "       (\"Sarah\",\"Marketing\"),\n",
        "       (\"David\",\"Engineering\")]\n",
        "cols2=[\"Name\",\"Department\"]\n",
        "\n",
        "empdf = spark.createDataFrame(data = emp, schema = cols1)\n",
        "condf = spark.createDataFrame(data = con, schema = cols2)\n",
        "\n",
        "empdf.filter(empdf.Department==\"Engineering\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9GMAHDefrCS"
      },
      "source": [
        "### **b)**\n",
        "Map the names of contractors to uppercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5CTN-Wbfqap",
        "outputId": "7827b655-e7c1-4630-99b4-bd173f8fa4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|upper(Name)|\n",
            "+-----------+\n",
            "|       MIKE|\n",
            "|      SARAH|\n",
            "|      DAVID|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import upper\n",
        "condf.select(upper(condf[\"Name\"])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C58aOK47sJsu"
      },
      "source": [
        "### **c)**\n",
        "Find all distinct individuals across both datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lckVdHOCsK95",
        "outputId": "085f37d4-6ead-46b6-d49e-bd08b71ef396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "| Name|\n",
            "+-----+\n",
            "|  Bob|\n",
            "| John|\n",
            "|Alice|\n",
            "| Emma|\n",
            "|Sarah|\n",
            "| Mike|\n",
            "|David|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ename=empdf.select(empdf[\"Name\"])\n",
        "cname=condf.select(condf[\"Name\"])\n",
        "distinctnames=ename.union(cname).distinct()\n",
        "distinctnames.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGnt_grCtzbY"
      },
      "source": [
        "### **d)**\n",
        "Combine the names of employees and contractors into a single list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWAEVf2ytyzO",
        "outputId": "e79016cf-98ab-4fe4-a4f0-688ca4142a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|  collect_list(Name)|\n",
            "+--------------------+\n",
            "|[Bob, John, Alice...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import collect_list\n",
        "List=distinctnames.select(collect_list(distinctnames.Name)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4mxaVTh0Gw8"
      },
      "source": [
        "### **e)**\n",
        "Find the intersection of employees and contractors who work in the \"Sales\" department."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oEMsQTz0Gb_",
        "outputId": "2d00df6b-6047-4d89-f36c-5aada0477dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+\n",
            "|Name|Department|Name|Department|\n",
            "+----+----------+----+----------+\n",
            "+----+----------+----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joindf=empdf.join(condf,empdf.Department==condf.Department)\n",
        "joindf.filter(empdf.Department==\"Sales\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67vq4YXOh20"
      },
      "source": [
        "## **Q No 4 :**\n",
        "You need to perform the following tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3kXzQGTOkKW",
        "outputId": "908818b6-8ab4-4c91-f515-851e79c23754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Q4.csv\n"
          ]
        }
      ],
      "source": [
        "%%file Q4.csv\n",
        "1,100,500.0\n",
        "2,75,300.0\n",
        "3,150,750.0\n",
        "1,80,400.0\n",
        "4,200,1000.0\n",
        "2,120,600.0\n",
        "3,90,450.0\n",
        "5,50,250.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNlNbh922xEF"
      },
      "source": [
        "### **a)**\n",
        "Find the total revenue generated from each product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A4oqBcX2wQX"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q4.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],word[2]))\n",
        "\n",
        "wordcount = words2.reduceByKey(lambda x, y:float(x)  +float(y))\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ4_1.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQOXlfoF4s_9"
      },
      "source": [
        "### **b)**\n",
        "Calculate the average revenue per unit for each product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xP0f2EMF4uiA"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q4.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],(word[2],word[1])))\n",
        "\n",
        "word3 = words2.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
        "words4=word3.mapvalue(lambda x:x[0]/x[1])\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ4_2.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAusTxU4nDec"
      },
      "source": [
        "### **c)**\n",
        "Find the top 5 products with the highest total revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NKsbEQrnFGh"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q4.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],(word[2],word[1])))\n",
        "\n",
        "word3 = words2.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).sort()\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ4_3.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGUGYiNYngnp"
      },
      "source": [
        "### **d)**\n",
        "Find the products with zero revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUZZl8dMniLT"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q4.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],word[2]))\n",
        "\n",
        "wordcount = words2.reduceByKey(lambda x, y:float(x)  +float(y))\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ4_4.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0F7RBYjnsV1"
      },
      "source": [
        "### **e)**\n",
        "Calculate the total revenue generated from products with a quantity sold greater than 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4O2f2ownyWm"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "Data = sc.textFile(\"Q4.csv\")\n",
        "\n",
        "# Split each line into words\n",
        "words = Data.flatMap(lambda line: line.split(\"\\n\"))\n",
        "words1= words.map(lambda line: line.split(\",\"))\n",
        "words2=words1.map(lambda word: (word[0],(word[2],word[1])))\n",
        "\n",
        "word3 = words2.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
        "\n",
        "# # Save the word counts to a text file\n",
        "wordcount.saveAsTextFile(\"outfileQ4_5.csv\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
