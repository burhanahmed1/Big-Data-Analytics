{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGT5Cx7jG0BO",
        "outputId": "1b9ec401-57ac-4b5f-ba8e-4a8910891f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=cc941e83bc572655da925a7f5afc7cfab8f75ce28dc747ea74d90a3066c79b14\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AATOJoOARBr"
      },
      "source": [
        "## **Q 1 :**\n",
        "Show the description of data.csv using pyspark.sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZtgC3BqG_Z7",
        "outputId": "d45311c5-8599-4863-eb55-b0b1a04b046c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing data.csv\n"
          ]
        }
      ],
      "source": [
        "%%file data.csv\n",
        "City,Temp1,Temp2,Temp3,Temp4,Temp5\n",
        "Hayward,71,69,71,71,72\n",
        "Baumholder,46,42,40,37,39\n",
        "Alexandria,50,48,51,53,44\n",
        "Melbourne,88,101,85,77,74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDlI1n1iaA0h",
        "outputId": "ceda69da-cb27-42e1-ba49-f57723d6a7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+\n",
            "|      City|Temp1|Temp2|Temp3|Temp4|Temp5|\n",
            "+----------+-----+-----+-----+-----+-----+\n",
            "|   Hayward|   71|   69|   71|   71|   72|\n",
            "|Baumholder|   46|   42|   40|   37|   39|\n",
            "|Alexandria|   50|   48|   51|   53|   44|\n",
            "| Melbourne|   88|  101|   85|   77|   74|\n",
            "+----------+-----+-----+-----+-----+-----+\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Temp1: integer (nullable = true)\n",
            " |-- Temp2: integer (nullable = true)\n",
            " |-- Temp3: integer (nullable = true)\n",
            " |-- Temp4: integer (nullable = true)\n",
            " |-- Temp5: integer (nullable = true)\n",
            "\n",
            "+----------+\n",
            "|avg(Temp1)|\n",
            "+----------+\n",
            "|     63.75|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Creating DataFrame\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load text file into DataFrame\n",
        "text_df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the loaded DataFrame\n",
        "text_df.show()\n",
        "\n",
        "# Show metadata\n",
        "text_df.printSchema()\n",
        "\n",
        "# # Calculate the average of Temp1 column\n",
        "avg_temp = text_df.select(avg(\"Temp1\")).show()\n",
        "\n",
        "# No need to stop SparkContext when using SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K14XA7LaYOr"
      },
      "source": [
        "## **Q 2 :**\n",
        "**You are given a file containing marks obtained by each student in different\n",
        "assessments in the form (marks, StudentNo)**\n",
        "Find the average marks obtained by each student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ikZeou62Hgoo"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "data=sc.parallelize([(8,\"S1\"),(2,\"S2\"),(7,\"S3\"),(4,\"S1\"),(5,\"S2\"),(2,\"S1\"),(5,\"S4\")])\n",
        "rdd=data.map(lambda x:(x[1],x[0]))\n",
        "rdd1=rdd.mapValues(lambda y: (y,1))\n",
        "rdd2=rdd1.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
        "avg=rdd2.mapValues(lambda x: (x[0]/x[1]))\n",
        "avg.collect()\n",
        "avg.saveAsTextFile(\"out5\")\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We have a huge data file of FAST students. The file consists of the roll number of the\n",
        "FAST students, followed by the subject code and the student grade in that subject**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_m0_YfBOHG",
        "outputId": "99c8a62b-8f65-49f9-89e2-0235de26f0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing students.txt\n"
          ]
        }
      ],
      "source": [
        "%%file students.txt\n",
        "L22-2100 DB D\n",
        "K21-1601 SE F\n",
        "I21-1601 OS F\n",
        "K21-1702 DS B\n",
        "L21-1705 OS A\n",
        "L22-2101 DB D\n",
        "K21-1601 OS F\n",
        "L21-1601 SE F\n",
        "L21-1702 SE B\n",
        "L21-1705 DB A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Q 3 :**\n",
        "Select the records of students from the Lahore campus. Display a few records and print\n",
        "the count of the students from Lahore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHHTL0XOAaJE",
        "outputId": "a9f6f90e-be91-49a5-93fc-7561462fcc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "data=sc.textFile(\"students.txt\")\n",
        "lines=data.filter(lambda x: x.startswith('L'))\n",
        "lines.saveAsTextFile(\"out7\")\n",
        "count_lhr=lines.count()\n",
        "print(count_lhr)\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOwXtqKAAa5V"
      },
      "source": [
        "## **Q 4 :**\n",
        "Find records of the students from the year in the range of 1995- 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VhP3Q-K8EOTA"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "data=sc.textFile(\"students.txt\")\n",
        "lines=data.filter(lambda x: int(x[1:3]) >= 95 or int(x[1:3]) <= 18)\n",
        "lines.saveAsTextFile(\"out9\")\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwr0gTFvINWv"
      },
      "source": [
        "## **Q 5 :**\n",
        "Display the count of students on each Campus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fq3C7sBEogH",
        "outputId": "3ae666f8-6986-4dd6-8bd6-b4ea1d5c81a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lahore Campus:  6\n",
            "Karachi Campus:  3\n",
            "Islamabad Campus:  1\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "data=sc.textFile(\"students.txt\")\n",
        "lhrCamp=data.filter(lambda x: 'L' in x)\n",
        "print('Lahore Campus: ',lhrCamp.count())\n",
        "karaCamp=data.filter(lambda x: 'K' in x)\n",
        "print('Karachi Campus: ',karaCamp.count())\n",
        "IslCamp=data.filter(lambda x: 'I' in x)\n",
        "print('Islamabad Campus: ',IslCamp.count())\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqSIhyqLK8fG"
      },
      "source": [
        "## **Q 6 :**\n",
        "Remove all the duplicate rows from input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lZ2BGLVMLQUq"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "data=sc.textFile(\"students.txt\")\n",
        "remove_duplicate=data.distinct()\n",
        "remove_duplicate.collect()\n",
        "remove_duplicate.saveAsTextFile(\"out10\")\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dJZ829EMNSA"
      },
      "source": [
        "## **Q 7 :**\n",
        "Find the minimum and maximum grades in each subject. The ordering of grades is as\n",
        "follows A >B >C >D >F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raxP-H8HZTX9"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"GradeAnalysis\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "data=sc.textFile(\"students.txt\")\n",
        "\n",
        "# Split each line into words\n",
        "lines = data.map(lambda line: line.split())\n",
        "\n",
        "# Extract grades from each record\n",
        "grades = lines.map(lambda x: x[2])\n",
        "\n",
        "# Convert grades to integers\n",
        "grades_int = grades.map(lambda x: int(x))\n",
        "\n",
        "# Calculate minimum and maximum grades\n",
        "min_grade = grades_int.min()\n",
        "max_grade = grades_int.max()\n",
        "\n",
        "# Print the results\n",
        "print(\"Minimum Grade:\", min_grade)\n",
        "print(\"Maximum Grade:\", max_grade)\n",
        "\n",
        "grades_int.saveAsTextFile(\"out20\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnvTMmzbL55"
      },
      "source": [
        "## **Q 8 :**\n",
        "For each student, compute the GPA. Assume only five grades (Grade A GPA=4, Grade B\n",
        "GPA=3, Grade C GPA 2, Grade D GPA 1, and Grade F GPA=0)\n",
        "\n",
        "Convert grades to GPA as mentioned above and find the average GPA of each Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3-6DOEJbLf8",
        "outputId": "5770bcf6-2d2d-43c9-e469-e2ac0760a746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student GPA:\n",
            "L22-2100: 1.0\n",
            "K21-1601: 0.0\n",
            "I21-1601: 0.0\n",
            "K21-1702: 3.0\n",
            "L21-1705: 4.0\n",
            "L22-2101: 1.0\n",
            "L21-1601: 0.0\n",
            "L21-1702: 3.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object\n",
        "conf = SparkConf().setAppName(\"StudentGPA\").setMaster(\"local\")\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Load data from text file\n",
        "data = sc.textFile(\"students.txt\")\n",
        "\n",
        "# Split each line into words\n",
        "lines = data.map(lambda line: line.split())\n",
        "\n",
        "# Convert grades to GPA values\n",
        "def grade_to_gpa(grade):\n",
        "    if grade == \"A\":\n",
        "        return 4\n",
        "    elif grade == \"B\":\n",
        "        return 3\n",
        "    elif grade == \"C\":\n",
        "        return 2\n",
        "    elif grade == \"D\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Compute GPA for each student\n",
        "student_gpa = lines.map(lambda x: (x[0], (grade_to_gpa(x[2]), 1)))\n",
        "student_gpa = student_gpa.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "student_gpa = student_gpa.mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "# Print the result\n",
        "print(\"Student GPA:\")\n",
        "for student, gpa in student_gpa.collect():\n",
        "    print(f\"{student}: {gpa}\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
